{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Overview\n",
        "\n",
        "This assignment is built on the skills and techniques introduced in previous assignments, but completion (partially or entirely) of any previous assignment is not required to complete this work.\n",
        "\n",
        "The previous assignment ended with word similarity and word analogy tests. Here, we take the next step and explore sentence similarity: students will learn how the representations of smaller language units (e.g., words) can be composed to form representations of larger units (e.g., sentences) using deep learning. Specifically, the assignment introduces\n",
        "1. the **task** of measuring semantic textual similarity (STS),\n",
        "2. a popular **dataset** of [Sentences Involving Compositional Knowledge (SICK)](https://zenodo.org/records/2787612),\n",
        "3. **fine-tuning** pretrained embeddings for a specific task, and\n",
        "4. how regression (or regression-like) tasks use **correlation statistics for evaluation**."
      ],
      "metadata": {
        "id": "1S_QRK2EvSJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Technical Overview of Model Architecture\n",
        "\n",
        "Modeling semantic textual similarity is complicated by the ambiguity and variability of linguistic expressions. To tackle this, you will develop and test a model comprising two components:\n",
        "\n",
        "1. A sentence model for converting a sentence into a representation for similarity measurement. This is a convolutional neural network (CNN) architecture with multiple types of convolution and pooling, designed to capture different granularities of information.\n",
        "2. A similarity measurement layer using structured similarity measurements, which compare local regions of sentence representations (obtained from the sentence model).\n",
        "\n",
        "This approach involves two subnetworks, each processing a sentence (in parallel). The subnetworks share all their weights, and are eventually joined by a similarity measurement layer. This is followed by a fully connected layer for the final similarity score output. This kind of an architecture is called a *Siamese network* or *twin network* in NLP research literature.\n",
        "\n",
        "> **[Schematic diagram of a twin network](https://drive.google.com/file/d/1sqS8n145QCEjxdBo6Ztlrjyf0ahoF8eJ/view?usp=drive_link)**\n",
        "\n",
        "*Make sure to understand the conceptual layout shown in the above schematic diagram before you proceed.*"
      ],
      "metadata": {
        "id": "RJ-5LPEW4IxJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Semantic Textual Similarity (STS): Technical Details and Programming\n"
      ],
      "metadata": {
        "id": "W73cXOnL60AP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 The SICK Dataset\n",
        "\n",
        "You are going to use a very well-known corpus called [the SICK (Sentences Involving Compositional Knowledge) dataset](https://zenodo.org/records/2787612). It includes information other than semantic similarity of sentences, but for the purposes of this assignment, you can ignore those additional properties of this corpus.\n",
        "\n",
        "So, let us first obtain the corpus."
      ],
      "metadata": {
        "id": "PZpenv21-wkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://zenodo.org/records/2787612/files/SICK.zip"
      ],
      "metadata": {
        "id": "4o3oQiR17udm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "462a3f82-e312-4057-8e9b-99afd6c16966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-30 00:00:57--  https://zenodo.org/records/2787612/files/SICK.zip\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.79.172, 188.184.103.159, 188.184.98.238, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.79.172|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 217584 (212K) [application/octet-stream]\n",
            "Saving to: ‘SICK.zip’\n",
            "\n",
            "SICK.zip            100%[===================>] 212.48K   591KB/s    in 0.4s    \n",
            "\n",
            "2024-04-30 00:00:58 (591 KB/s) - ‘SICK.zip’ saved [217584/217584]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a `.zip` archive, so we need to extract it."
      ],
      "metadata": {
        "id": "PjFA7hWV8CRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "with ZipFile('SICK.zip', 'r') as z:\n",
        "    z.extractall('sick_dataset')"
      ],
      "metadata": {
        "id": "MGKZH5vK8JWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should be able to see the extracted corpus using the `Files` icon on the left sidebar here on Colab. The corpus resides in the `sick_dataset` folder, and contains a `readme.txt` and a `SICK.txt`.\n",
        "\n",
        "By default, this should be located in your `/content` folder on Colab. You can/should verify this using the `!pwd`, `!cd`, and `!ls` commands.\n",
        "\n",
        "*Before moving forward in this assignment, check the structure of the data and understand what it provides.*"
      ],
      "metadata": {
        "id": "YjDggRFt8fl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 The `torchtext` package\n",
        "\n",
        "This package consists of data processing utilities for natural language processing. You are being introduced to this package through this assignment with the expectation that you will find it useful not just in this assignment, but in future work related to NLP. It has the added advantage of being extremely well-integrated with the wider PyTorch project."
      ],
      "metadata": {
        "id": "TnsS80MsWQ7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext"
      ],
      "metadata": {
        "id": "sIIoDIQCW_w5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0d22968-3915-44af-919b-8d6dfc786cec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: torch==2.2.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.25.2)\n",
            "Requirement already satisfied: torchdata==0.7.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.1->torchtext)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.1->torchtext)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.1->torchtext)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.1->torchtext)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.1->torchtext)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.1->torchtext)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.1->torchtext)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.1->torchtext)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.1->torchtext)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.1->torchtext)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.1->torchtext)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (2.2.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext) (2.0.7)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->torchtext)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.1->torchtext) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.1->torchtext) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Utility functions and Lexical Similarity\n",
        "\n",
        "Here, we provide you with a utility function that creates a dictionary and one function that provides a few features based on lexical overlap.\n",
        "\n",
        "The use of these functions is highly recommended (but not mandatory), as these features are known to improve the performance in semantic similarity detection.\n",
        "\n",
        "You are free to add other utility functions that compute specific features or create various dictionaries (for maintaining indices, or other mappings required by your implementation).\n",
        "- **Always provide a docstring with any function you add, and also include type hints so that the data types are obvious to anyone using your code**."
      ],
      "metadata": {
        "id": "CYAZxnbk-gtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords', quiet=True)\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def pairwise_word2doc_frequency(sentence_list_1: list[str], sentence_list_2: list[str]) -> dict[str, int]:\n",
        "    \"\"\"\n",
        "    Calculate the document frequency of each unique word from two lists of sentences.\n",
        "\n",
        "    This function counts how many \"documents\" (in this context, a pair of sentences) each unique word appears in.\n",
        "    Each pair of sentences from the two lists is considered as a separate \"document\". If a word occurs in either\n",
        "    sentence of the pair, it is counted once for that pair.\n",
        "\n",
        "    Args:\n",
        "    sentence_list_1 (list[str]): The first list of sentences.\n",
        "    sentence_list_2 (list[str]): The second list of sentences. It should be of the same length as sentence_list_1.\n",
        "\n",
        "    Returns:\n",
        "    dict[str, int]: A dictionary where keys are the unique words and values are the number of \"documents\" in which the\n",
        "                    word appears.\n",
        "\n",
        "    Raises:\n",
        "    ValueError: If the input lists have different lengths.\n",
        "    \"\"\"\n",
        "    if len(sentence_list_1) != len(sentence_list_2):\n",
        "        raise ValueError(\"Sentence lists have different lengths.\")\n",
        "    word2doc_counts = defaultdict(int)\n",
        "    for s1, s2 in zip(sentence_list_1, sentence_list_2):\n",
        "        uniquetokens = set(s1) | set(s2)\n",
        "\n",
        "        for t in uniquetokens:\n",
        "            word2doc_counts[t] += 1\n",
        "            #print(word2doc_counts)\n",
        "    return word2doc_counts\n",
        "\n",
        "def pairwise_lexical_overlap_features(sentence_list_1: list[str], sentence_list_2: list[str],\n",
        "                                      word2doc_counts: dict[str, int]) -> list[list[float]]:\n",
        "    \"\"\"\n",
        "    Calculate various lexical overlap features between two lists of tokenized sentences.\n",
        "\n",
        "    This function computes four types of lexical overlap features for each pair of sentences:\n",
        "    1. Basic overlap: The proportion of overlapping tokens in the two sentences.\n",
        "    2. IDF-weighted overlap: The inverse document frequency (IDF) weighted overlap score.\n",
        "    3. Content-only overlap: The basic overlap excluding stopwords.\n",
        "    4. Content-only IDF-weighted overlap: The IDF-weighted overlap score excluding stopwords.\n",
        "\n",
        "    Args:\n",
        "        sentence_list_1 (list[str]): The first list of sentences to be analyzed.\n",
        "        sentence_list_2 (list[str]): The second list of sentences to be analyzed.\n",
        "        word2doc_counts (dict[str, int]): A dictionary mapping tokens to their document frequency across a corpus.\n",
        "\n",
        "    Returns:\n",
        "        list[list[float]]: A list of lists, where each sublist contains four float values for each sentence-pair:\n",
        "                           [overlap, IDF-weighted overlap, content-only overlap, content-only IDF-weighted overlap]\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If `sentence_list_1` and `sentence_list_2` have different lengths.\n",
        "    \"\"\"\n",
        "    if len(sentence_list_1) != len(sentence_list_2):\n",
        "        raise ValueError(\"Sentence lists have different lengths.\")\n",
        "\n",
        "    stopwords_set = set(stopwords.words('english'))\n",
        "    num_docs = len(sentence_list_1)\n",
        "    overlap_features = []\n",
        "    for s1, s2 in zip(sentence_list_1, sentence_list_2):\n",
        "        tokens_a_set, tokens_b_set = set(s1), set(s2)\n",
        "        intersection = tokens_a_set & tokens_b_set\n",
        "        overlap = len(intersection) / (len(tokens_a_set) + len(tokens_b_set))\n",
        "\n",
        "        #Needed to switch to check divide by 0 later\n",
        "        intersection_with_counts = [t for t in intersection if t in word2doc_counts and word2doc_counts[t] != 0]\n",
        "        idf_intersection = sum(np.log(num_docs / word2doc_counts[t]) for t in intersection_with_counts)\n",
        "\n",
        "        tokens_a_contentset = set(t for t in s1 if t not in stopwords_set)\n",
        "        tokens_b_contentset = set(t for t in s2 if t not in stopwords_set)\n",
        "        intersection_content = tokens_a_contentset & tokens_b_contentset\n",
        "        overlap_content = len(intersection_content) / (len(tokens_a_contentset) + len(tokens_b_contentset))\n",
        "\n",
        "        #Needed to switch to check divide by 0 later\n",
        "        intersection_content_with_counts = [t for t in intersection_content if t in word2doc_counts and word2doc_counts[t] != 0]\n",
        "        idf_intersection_content = sum(np.log(num_docs / word2doc_counts[t]) for t in intersection_content_with_counts)\n",
        "\n",
        "        #Check divide by 0, if 0 set to 0 else complete\n",
        "        if len(intersection_with_counts) >0:\n",
        "            idf_weighted_overlap = idf_intersection / (len(tokens_a_set) + len(tokens_b_set))\n",
        "        else:\n",
        "            idf_weighted_overlap = 0.0\n",
        "        if len(intersection_content_with_counts) >0:\n",
        "            idf_weighted_overlap_content = idf_intersection_content / (len(tokens_a_contentset) + len(tokens_b_contentset))\n",
        "        else:\n",
        "            idf_weighted_overlap_content =0.0\n",
        "        overlap_features.append([overlap, idf_weighted_overlap, overlap_content, idf_weighted_overlap_content])\n",
        "    return overlap_features"
      ],
      "metadata": {
        "id": "8C8qQo3V_nLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before going any further, we must fix random seeds for random number generation throughout the remainder of this assignment. This ensures that the experiments are reproducible."
      ],
      "metadata": {
        "id": "ESkSklrlfqfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "\n",
        "# Do not change the seed value and any line where this value is used for settings.\n",
        "# Changing the seed values may prevent your results from being reproduced if needed.\n",
        "\n",
        "SEED = 1234\n",
        "DATA_SPLIT_SEED = 99\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device.type == 'cuda':\n",
        "    torch.cuda.manual_seed(SEED)"
      ],
      "metadata": {
        "id": "vVCM1sCcf8R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 The SICK Dataset Object in Python\n",
        "\n",
        "Here, you are given the class to represent the SICK dataset. Some preprocessing functionality is also included. PyTorch allows for [map-style datasets](https://pytorch.org/docs/stable/data.html#map-style-datasets), which is the approach taken in this assignment.\n",
        "\n",
        "You are encouraged to add methods and/or enhance the structure of this dataset in any way, as long as the additional code does not use a prohibited library or package. But please remember that the primary objective of this assignment is to understand convolutional neural networks (CNNs) and semantic representation of sentences. Your enhancements should be guided by those goals. Otherwise, you run the risk of overinvesting in this portion of the assignment for diminished returns!\n",
        "\n",
        "> A standard recommendation from the teaching staff is that once you understand the `SickDataset` class in its given form, you should move on to the CNN and its filters (next section). There, you may think of enhacing the `SickDataset` class in certain ways so that the class' structure integrates smoothly with how you want to use the CNN architecture.\n"
      ],
      "metadata": {
        "id": "ApFIxdfPUeO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class SickDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset class for loading and processing the SICK dataset.\n",
        "\n",
        "    This class handles loading the SICK dataset from a specified path and provides methods for processing the data,\n",
        "    splitting it into training, development, and test sets, and accessing individual instances.\n",
        "\n",
        "    Attributes:\n",
        "        path (str): The path to the directory containing the SICK dataset files.\n",
        "        tokenizer (callable): The tokenizer function used to process text data.\n",
        "        fields (list): A list specifying how each column in the dataset should be processed.\n",
        "        instances (list): A list containing dictionaries, each representing an instance in the dataset.\n",
        "                          Each dictionary contains the processed data for a single instance.\n",
        "\n",
        "    Methods:\n",
        "        __len__(): Returns the total number of instances in the dataset.\n",
        "        __getitem__(idx): Returns the instance at the specified index.\n",
        "        splits(): Splits the dataset into training, development, and test sets.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        self.tokenizer = get_tokenizer(\"basic_english\")\n",
        "        self.fields = [('id', None), ('sentence_1', self.tokenizer), ('sentence_2', self.tokenizer),\n",
        "                       ('external_features', None), ('label', float)]\n",
        "        self.instances = self._process_data()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.instances)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.instances[idx]\n",
        "\n",
        "    def __create_instance(self, row):\n",
        "        return {\n",
        "            'id': row['pair_ID'],\n",
        "            'sentence_1': row['sentence_A'],\n",
        "            'sentence_2': row['sentence_B'],\n",
        "            'external_features': row['overlap_features'],\n",
        "            'label': float(row['relatedness_score'])\n",
        "        }\n",
        "\n",
        "    def _process_data(self):\n",
        "        corpus_df = pd.read_csv(os.path.join(self.path, 'SICK.txt'), sep='\\t')\n",
        "        remove_trailing_space = lambda s : self.tokenizer(s.rstrip())\n",
        "        sentence_1_list = corpus_df['sentence_A'].apply(remove_trailing_space).tolist()\n",
        "        sentence_2_list = corpus_df['sentence_B'].apply(remove_trailing_space).tolist()\n",
        "\n",
        "        self.word2doc_counts = pairwise_word2doc_frequency(sentence_1_list, sentence_2_list)\n",
        "        corpus_df['overlap_features'] = pairwise_lexical_overlap_features(sentence_1_list, sentence_2_list,\n",
        "                                                                          self.word2doc_counts)\n",
        "        instances = corpus_df.apply(self.__create_instance, axis=1).tolist()\n",
        "\n",
        "        return instances\n",
        "\n",
        "    def splits(self) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Split the dataset into training (70%), development (10%), and test (20%) sets.\n",
        "\n",
        "        Returns:\n",
        "            tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]: A tuple containing three DataFrames, in the order train_df,\n",
        "                                                             dev_df, and test_df.\n",
        "        \"\"\"\n",
        "        entire_df = pd.DataFrame(self.instances)\n",
        "\n",
        "        train_df, other = train_test_split(entire_df, test_size=0.3, random_state=DATA_SPLIT_SEED)\n",
        "        dev_df, test_df = train_test_split(other, test_size=2/3, random_state=DATA_SPLIT_SEED)\n",
        "\n",
        "        return train_df, dev_df, test_df"
      ],
      "metadata": {
        "id": "CwbIsSxAXo70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Model Description\n",
        "\n",
        "A **convolutional neural network (CNN)** is a *regularized* feed-forward neural network that learns useful features automatically through the use of **filters** (a process that removes some unwanted components or features from an input).\n",
        "- A **convolution** is the process of applying a filter to text data to extract features from it. The original idea comes from image processing, where convolutions are applied to 2D grids. In NLP, we apply a filter to a sequence of words or characters.\n",
        "\n",
        "You will use a CNN to model each sentence, by using two types of convolution filters. The intention is to model two different perspectives of the semantics of a sentence. You will also explore multiple types of pooling.\n",
        "\n",
        "We view the input as a sequence of tokens where nearby tokens are very likely correlated. Thus, we consider a sentence $S \\in \\mathbb{R}^{l \\times d}$ as a sequence of $l$ input words, and each word represented by a $d$-dimensional embedding.\n",
        "\n",
        "We will need to introduce some notation for a technical description of the model. To keep things as similar as possible to our textbook,\n",
        "\n",
        "- $S_i \\in \\mathbb{R}^d$ will denote the embedding of the $i^{\\text{th}}$ word in the sequence\n",
        "- $S_{i:j}$ will denote the concatenation of embeddings from word $i$ up to and including word $j$.\n",
        "- The $k^{\\text{th}}$ dimension will be denoted by $[k]$ in the supersccript. That is, $S_i^{[k]}$ is the dimension $k$ in the representation of word $i$ in our sentence. Similarly, $S_{i:j}^{[k]}$ is the vector of the values in the $k^{\\text{th}}$ dimension of words $i$ to $j$.\n"
      ],
      "metadata": {
        "id": "YVDdaMZdjfih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5.1 Convolution filters\n",
        "\n",
        "We define a convolution filter $F = \\langle ws, w_F, b_F, h_F \\rangle$ as a tuple of size 4, comprising\n",
        "1. $ws$, the width of the sliding window,\n",
        "2. $w_F$, the weight vector for the filter (this is a vector in $\\mathbb{R}^{ws \\times d}$),\n",
        "3. a real-valued scalar bias $b_F$, and\n",
        "4. a nonlinear activation function $h_F$.\n",
        "\n",
        "When the above filter is applied to a sentence $S$, it computes the inner product between $w_F$ and each possible window of length $ws$ in the sentence $S$. Then, as with any feedforward neural network, we add the bias and apply the activation function. Thus, the output is a vector $\\mathbf{o}_F \\in \\mathbb{R}^{1+l-ws}$ given by\n",
        "$$\\mathbf{o}_F = \\langle h_F(w_F⋅S_{i:i+ws-1}+b_F)\\rangle_{i=1}^{1+l-ws}$$\n",
        "\n",
        "First, you will need some pretrained embeddings to serve as initial $d$-dimensional word embeddings. For this, let us resort to something you have already seen in the previous assignment: GloVe embeddings."
      ],
      "metadata": {
        "id": "Vi9k-KJ6Za4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "metadata": {
        "id": "D4JAVx3eYN6M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6102a10b-de85-4d9a-ec93-decc69aa1937"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-30 00:02:23--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-04-30 00:02:23--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
            "\n",
            "2024-04-30 00:05:03 (5.16 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you have the following embeddings:\n",
        "\n",
        "- `glove.6B.50d.txt`\n",
        "- `glove.6B.100d.txt`\n",
        "- `glove.6B.200d.txt`\n",
        "- `glove.6B.300d.txt`\n",
        "\n",
        "You can either use the `gensim` library as shown in the previous assignment, or use these pretrained embeddings directly as follows:"
      ],
      "metadata": {
        "id": "Cpn8647JZ_L6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_embeddings(file_path):\n",
        "    embeddings_index = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    return embeddings_index\n",
        "\n",
        "# Example use:\n",
        "embeddings_file = 'glove.6B.50d.txt'\n",
        "glove_embeddings = load_glove_embeddings(embeddings_file)\n",
        "\n",
        "if 'fox' in glove_embeddings:\n",
        "    fox_embedding = glove_embeddings['fox']\n",
        "    print(\"Embedding vector for 'fox':\", fox_embedding)\n",
        "else:\n",
        "    print(\"Embedding vector for 'fox' not found.\")"
      ],
      "metadata": {
        "id": "Uowpq0m2cQcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a00a34c-1318-4877-9447-56fcb121b5f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding vector for 'fox': [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
            " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
            "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
            "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
            "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
            " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
            " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
            "  1.5064  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With such pretrained embeddings, and the technical details described above, it's time to implement the convolution filter. The start of this portion is given to you below. You are free to add and/or modify fields to make this filter class richer, if you want/need (and you will almost certainly need to add more methods)."
      ],
      "metadata": {
        "id": "ALRXeAcjdIQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class ConvolutionalFilter(nn.Module):\n",
        "    def __init__(self, window_size, embedding_dim, activation_fn=nn.Tanh()):\n",
        "        super(ConvolutionalFilter, self).__init__()\n",
        "        self.window_size = window_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.weight = nn.Parameter(torch.randn(window_size, embedding_dim))\n",
        "        self.bias = nn.Parameter(torch.randn(1))\n",
        "        self.activation_function = activation_fn\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        # In a PyTorch nn.Module, the forward function defines the forward pass computation of the neural network. This\n",
        "        # function describes how input data is processed through the layers of the network to produce the output.\n",
        "        # Usually, it consists of\n",
        "        # 1. The input data (usually a tensor) is passed into the forward function as an argument.\n",
        "        # 2. Then, you define the computation graph by specifying how the input data flows forward through the layers\n",
        "        #    of your network.\n",
        "        # 3. You apply operations defined by the layers (e.g., convolutional transformation, activation functions,\n",
        "        #    pooling, etc.) to the input data successively, forming the forward pass of the network.\n",
        "        # 4. Finally, the forward function returns the output of the network after processing the input data through all\n",
        "        #    the layers.\n",
        "        # TODO\n",
        "\n",
        "        #Unfold sentence to get sentence length - window size + 1 windows of the sentence embeddings\n",
        "        sentence_unfolded = sentence.unfold(0, self.window_size, 1).transpose(1, 2)\n",
        "\n",
        "        #Compute product of windows and weights transformed\n",
        "        out = torch.matmul(sentence_unfolded, self.weight.t())\n",
        "        out += self.bias\n",
        "\n",
        "        #Sum output matrix to get convultion\n",
        "        out = torch.sum(out, dim=(1, 2))\n",
        "        #print(out)\n",
        "        #Finally do activation on result\n",
        "        out = self.activation_function(out)\n",
        "        #print(out.shape)\n",
        "        #do pooling outside\n",
        "        return out #Should be vector of shape - (length - window size + 1)\n",
        "\n",
        "# # Example use:\n",
        "# embeddings_file = 'glove.6B.50d.txt'\n",
        "# sentence = 'the quick brown jumped over the moon'\n",
        "\n",
        "# sentence_embeddings = []\n",
        "# for word in sentence.split():\n",
        "#   if word in glove_embeddings:\n",
        "#     word_embedding = glove_embeddings[word]\n",
        "#     sentence_embeddings.append(word_embedding)\n",
        "#   else:\n",
        "#     print('error')\n",
        "\n",
        "# sentence_tensor = torch.tensor(sentence_embeddings)\n",
        "# #print(sentence_tensor)\n",
        "# cf = ConvolutionalFilter(window_size=3, embedding_dim=50)\n",
        "\n",
        "# print(cf.forward(sentence_tensor))"
      ],
      "metadata": {
        "id": "hyaQWx4JzBfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5.2 Pooling\n",
        "\n",
        "Once you start looking into the details of the convolution filter, you will perhaps notice a discrepancy in the shape of certain intermediate results and the output vector as described. This is where **pooling** comes in.\n",
        "\n",
        "In networks like this, the output vector of a convolution filter is usually converted to a scalar, and this conversion is done using some sort of pooling. In this context, it makes sense for us to think of an operational object triple $(ws, p, S)$ that contains a convolution layer with width $ws$, uses a pooling function $p$, and operates on the sentence $S$. These operational triples model different perspectives on the semantics of a sentence. That is, such a \"perspective\" can be defined as\n",
        "\n",
        "$$\\{(ws, p, S) : p \\in \\{\\max, \\min, \\text{mean}\\}\\}$$\n",
        "\n",
        "**Why all these complicated things?**\n",
        "*We want each convolution layer to learn to recognize distinct phenomena of the input. This allows for richer modeling of compositional semantics. To this end, the design of the operational triples allows a pooling function to interact with its own underlying convolution layers independently.*\n",
        "\n",
        "For a perspective $(ws, p, S)$, with a convolution layer with $n$ filters, the output is a vector of length $n$, whose $i^\\text{th}$ entry is $p(\\mathbf{o}_{F_i})$, where $F_i$ is the $i^\\text{th}$ filter.\n",
        "\n",
        "Pooling is a common technique used in neural networks for reducing the spatial dimensions of tensors.\n",
        "\n",
        "**Max Pooling** is a pooling operation that takes the maximum value from each patch of the input tensor. It effectively downsamples the input tensor by retaining only the maximum value from each patch. In the simplest scenario, the entire tensor is a single patch, and it is downsampled to a single scalar -- the element in that tensor which had the highest value.\n",
        "\n",
        "**Mean Pooling**, also known as average pooling, computes the average value from each patch of the input tensor. It downsamples the input tensor by retaining the average value from each patch. In the simplest scenario, the entire tensor is downsampled to a single scalar, which is the average of all the elements in that tensor.\n",
        "\n",
        "**Min Pooling** computes the minimum value from each patch of the input tensor. It downsamples the input tensor by retaining the minimum value from each patch. In the simplest case, the entire tensor is replaced by a scalar -- its minimum element.\n",
        "\n",
        "These pooling operations are commonly used in convolutional neural networks (CNNs) and other types of neural network architectures to reduce the spatial dimensions of feature maps while retaining important information. They help in controlling overfitting and in extracting essential features from input data."
      ],
      "metadata": {
        "id": "aBZJM4OWexgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5.3 Structured Similarity Computation\n",
        "\n",
        "Recall the lexical overlap features provided to you initially as a utility function. You may or may not have used it in your code already. Now, however, is the time to look at it again.\n",
        "\n",
        "You will combine word representations through different pooling techniques and window sizes to create the input to the similarity computation layer in your Siamese network (i.e., the `ConvolutionalTwinNetwork` coming up soon). So, simply using cosine similarity is most probably not leveraging all that hard work!\n",
        "\n",
        "For both sentences in the input, you have used convolution filters. Some of these filters may have used max pooling, others may have used min or mean pooling. Similarly, multiple window sizes may have also been used.\n",
        "\n",
        "Now, build a similarity measurement function with the following properties:\n",
        "\n",
        "- it computes the similarity of the max-pooled regions of the representation of sentence 1 with the max-pooled regions of the representation of sentence 2 (and similarly for min-pooled and mean-pooled)\n",
        "- it computes a weighted sum of these regional similarities, where the weights are derived from the amount of lexical overlap\n",
        "- the final similarity measure abides by the basic rule of cosine similarity in that its range is $[-1, 1]$."
      ],
      "metadata": {
        "id": "0qXHRY9Gr4J9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# Write your structured similarity computation function here. This is the function you should use in the siamese network\n",
        "# code. Observe carefully how `nn.CosineSimilarity` works, and write your code in a similar form so that in your final\n",
        "# experiments, you can swap the standard cosine similarity with your custom function (i.e., this one) and see which one\n",
        "# works better.\n",
        "\n",
        "\n",
        "def structured_similarity(sentence1: str, sentence2: str, filtered_tokens1: torch.Tensor, filtered_tokens2: torch.Tensor, word2doc_counts: dict[str, int], device) -> float:\n",
        "    \"\"\"\n",
        "    Computes structured similarity between two sentences using lexical overlap\n",
        "    \"\"\"\n",
        "    #Get overlap should always be one list with 4 floats if given just 2 sentences\n",
        "    # print(sentence1)\n",
        "    # print(sentence2)\n",
        "    # print(word2doc_counts)\n",
        "    overlap_features = pairwise_lexical_overlap_features([sentence1.lower()], [sentence2.lower()], word2doc_counts)[0]\n",
        "    basic_overlap, IDF_weighted_overlap, content_only_overlap, content_only_IDF_weighted_overlap = overlap_features\n",
        "\n",
        "    #CosineSim between the pooled filters\n",
        "    pool_similarity = nn.CosineSimilarity(dim=0)(filtered_tokens1, filtered_tokens2)\n",
        "\n",
        "    #Combine features together (no weights for now so weights are just equal may add weights in fututre)\n",
        "    similarity = torch.tensor([pool_similarity, basic_overlap, IDF_weighted_overlap, content_only_overlap, content_only_IDF_weighted_overlap]).to(device).float()\n",
        "\n",
        "    #Get in range -1 to 1, still needs linear\n",
        "    similarity = torch.tanh(similarity)\n",
        "\n",
        "    return similarity\n",
        "\n"
      ],
      "metadata": {
        "id": "fDCnjjGpu4sN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5.4 Siamese Network\n",
        "\n",
        "Once you have finished the convolutional filter, the next step is to build a \"Siamese\" feedforward neural network whose input layer takes in two sentences.\n",
        "\n",
        "**How to handle sentences of varying lengths?** *Pad the shorter sentence with zero vectors.*\n",
        "\n",
        "PyTorch can be used to combine the following to create a Siamese neural network:\n",
        "\n",
        "- an input layer (with padding, as described above),\n",
        "- a hidden convolutional layer (which uses the convolutional filter described earlier),\n",
        "- a similarity computation layer,\n",
        "- a fully connected (linear) layer, and\n",
        "- the final scalar output, which is the \"relatedness score\" of the two sentences (recall that the gold-standard actual score is available from the dataset during training, and also for evaluation during testing).\n",
        "\n",
        "This component of the assignment requires you to build this network. You do **not** have to build it from scratch! You should use the flexibility of PyTorch and combine layers built using PyTorch.\n",
        "\n",
        "A skeleton code is given below. Please understand that this is simply the skeleton to show you the overall structure to be followed.\n",
        "\n"
      ],
      "metadata": {
        "id": "pGABQZgLi9RI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConvolutionalTwinNetwork(nn.Module):\n",
        "    def __init__(self, embedding_dim, vocab_size, filter_widths, num_filters, hidden_dim, similarity_measurement='structure', convolution_layer='our_conv', padding_size=15, device='cpu'):\n",
        "        super(ConvolutionalTwinNetwork, self).__init__()\n",
        "\n",
        "        #Embedding layer, choosing not to update glove embedding, just using weight to update instead, may switch later\n",
        "        #self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        self.glove_embeddings = load_glove_embeddings(f'glove.6B.{embedding_dim}d.txt')\n",
        "\n",
        "        self.padding_size = padding_size\n",
        "\n",
        "        self.window_sizes = filter_widths\n",
        "\n",
        "\n",
        "        #Need to create num_filters filters, will loop through all window sizes till all created\n",
        "        self.conv_layers = nn.ModuleList()\n",
        "\n",
        "        num_filters_made = 0\n",
        "        while num_filters_made < num_filters:\n",
        "          for window_size in self.window_sizes:\n",
        "              if num_filters_made < num_filters:\n",
        "                  num_filters_made +=1\n",
        "                  conv_layer = ConvolutionalFilter(window_size, self.embedding_dim, nn.Tanh())\n",
        "                  self.conv_layers.append(conv_layer)\n",
        "              else:\n",
        "                  break\n",
        "\n",
        "        print(self.conv_layers)\n",
        "\n",
        "        # imilarity measurement layer\n",
        "        self.similarity_measurement = similarity_measurement #updated to use either simularity\n",
        "\n",
        "        #Fully connected layer only needed for own similarity\n",
        "        self.fc = nn.Linear(5, 1) #5 for size of output from similarity function\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, sentence1, sentence2, word2doc_counts):\n",
        "        sentence1 = list(sentence1)\n",
        "        sentence2 = list(sentence2)\n",
        "        final_scaled_outputs = []\n",
        "\n",
        "        #Go through each sentence in a batch\n",
        "        for sen1, sen2 in zip(sentence1, sentence2):\n",
        "            pooled_outputs1 = []\n",
        "            pooled_outputs2 = []\n",
        "\n",
        "            #Get embeddings from helper function defined below, padding is done there too\n",
        "            embedded_sen1 = self.embed_sentence(sen1)\n",
        "            embedded_sen2 = self.embed_sentence(sen2)\n",
        "\n",
        "            # print(sen1)\n",
        "            # print(embedded_sen1.shape)\n",
        "            # print(sen2)\n",
        "            # print(embedded_sen2.shape)\n",
        "            # move to device, done in helper\n",
        "            # embedded_sen1 = embedded_sen1.to(self.device)\n",
        "            # embedded_sen2 = embedded_sen2.to(self.device)\n",
        "\n",
        "            #Put sentence 1 and 2 through all conv filters\n",
        "            conv_outputs1 = [conv(embedded_sen1) for conv in self.conv_layers]\n",
        "            conv_outputs2 = [conv(embedded_sen2) for conv in self.conv_layers]\n",
        "\n",
        "            #Pool each output to extract information in sentence\n",
        "            #Use this for similarity comparision\n",
        "            #Use every form of pooling, may cause issues but should extract different semantical info\n",
        "            for conv_output1, conv_output2 in zip(conv_outputs1, conv_outputs2):\n",
        "                max_pooled1 = torch.max(conv_output1, dim=-1)[0].unsqueeze(0)\n",
        "                avg_pooled1 = torch.mean(conv_output1, dim=-1).unsqueeze(0)\n",
        "                min_pooled1 = torch.min(conv_output1, dim=-1)[0].unsqueeze(0)\n",
        "\n",
        "                max_pooled2 = torch.max(conv_output2, dim=-1)[0].unsqueeze(0)\n",
        "                avg_pooled2 = torch.mean(conv_output2, dim=-1).unsqueeze(0)\n",
        "                min_pooled2 = torch.min(conv_output2, dim=-1)[0].unsqueeze(0)\n",
        "\n",
        "                #Combine all pooled output from one set of convooltuion outputs\n",
        "                pooled_outputs1.extend([max_pooled1,avg_pooled1,min_pooled1])\n",
        "                pooled_outputs2.extend([max_pooled2,avg_pooled2,min_pooled2])\n",
        "\n",
        "            #Merge together all poolings for comparision\n",
        "            pooled_outputs1_cat = torch.cat(pooled_outputs1,dim=0)\n",
        "            pooled_outputs2_cat = torch.cat(pooled_outputs2,dim=0)\n",
        "\n",
        "            #Similarity comparision can use just cosine or own function,\n",
        "            #Own fucntion should give better results\n",
        "            # print(self.similarity_measurement)\n",
        "\n",
        "            if self.similarity_measurement == 'structure':\n",
        "                similarity = structured_similarity(sen1, sen2, pooled_outputs1_cat, pooled_outputs2_cat, word2doc_counts, self.device)\n",
        "                similarity = self.fc(similarity)\n",
        "                activ_fn = nn.Tanh()\n",
        "                similarity = activ_fn(similarity)\n",
        "            else:\n",
        "                similarity = F.cosine_similarity(pooled_outputs1_cat, pooled_outputs2_cat, dim=0)\n",
        "            #print('sim: ', similarity, conv_output1, conv_output2)\n",
        "\n",
        "            #print('here')\n",
        "\n",
        "            #\n",
        "            # Apply fully connected layer\n",
        "            # output = self.fc(similarity)\n",
        "\n",
        "            # activ_fn = nn.Tanh()\n",
        "            # final_output = activ_fn(output)\n",
        "\n",
        "            #Scale to 1-5 so we can mesure against labels\n",
        "            final_scaled_output = (((similarity + 1) * 4) / 3) + 1\n",
        "\n",
        "            final_scaled_outputs.append(final_scaled_output)\n",
        "\n",
        "        #Meger all batch results together for one gradient update\n",
        "        final_scaled_outputs_tensor = torch.stack(final_scaled_outputs)\n",
        "        return final_scaled_outputs_tensor\n",
        "\n",
        "    def embed_sentence(self, sentence):\n",
        "      \"\"\"\n",
        "      Given a sentence uses glove embedding to get the embeddings for each word.\n",
        "      \"\"\"\n",
        "      #Get padding for sentence, paddding should always be greater than length since added extra\n",
        "      sentence_length = len(sentence.split())\n",
        "      amount_to_pad = self.padding_size - sentence_length\n",
        "\n",
        "      #Break sentence into words and get emebedding for each\n",
        "      #Using the word 'the' to as an unknown token since it is a common stop word\n",
        "      #Shouldn't change meaning of sentence and should work since if word not known not\n",
        "      #much we can do since not focused on getting own word embedding\n",
        "      #Will maybe experiment with using other stop words\n",
        "      embedded_sentence = []\n",
        "      for word in sentence.split():\n",
        "          word = word.lower()\n",
        "          if word in self.glove_embeddings:\n",
        "              word_embedding = self.glove_embeddings[word]\n",
        "          else:\n",
        "              word_embedding = self.glove_embeddings['the']\n",
        "          embedded_sentence.append(torch.tensor(word_embedding))\n",
        "\n",
        "      #Padding here, just adding 0s to fill up size\n",
        "      zero_array = torch.zeros(self.embedding_dim, dtype=torch.float32)\n",
        "      for _ in range(amount_to_pad):\n",
        "          embedded_sentence.append(zero_array)\n",
        "      embedded_sentence = torch.stack(embedded_sentence)\n",
        "\n",
        "      return embedded_sentence.to(self.device)\n"
      ],
      "metadata": {
        "id": "jZE7PPehquYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6 Training (fine-tuning)\n",
        "\n",
        "With your siamese network ready, you can now start modifying the pretrained vectors. This is easier than training a model from scratch, and typically happens much faster (even without a GPU runtime, the entire process should not take more than 1 - 1.5 hours).\n",
        "\n",
        "The training process is nothing new, so we won't repeat any code from the previous assignment here. The steps are:\n",
        "\n",
        "1. Define your Siamese network with pretrained embeddings\n",
        "2. Define your loss function\n",
        "3. Define/choose your optimizer (we recommend Adam, available in PyTorch)\n",
        "4. Create your training loop (in this assignment, you will not need more than 6-8 epochs; the use of small batch sizes such as 8 is recommended)\n",
        "\n",
        "By training your Siamese network with gold-standard labels, you will effectively be modifying the pretrained embeddings to become better suited to your specific task. This process is called **fine tuning**. In this assignment, you are thus fine-tuning GloVe embeddings to capture semantic similarity between sentences, and thus, you are also creating embeddings (think of the output of your convolution layer) that capture the meaning of sentences instead of individual words.\n",
        "\n",
        "One loss function you can use is the **mean squared error**. But, it turns out that there is better loss function for STS, called the **KL Divergence loss**. This is defined as follows:\n",
        "\n",
        "$\\mathcal{L}_{KL}(\\theta) = \\frac{1}{m}\\sum_{k=1}^m \\left(\\mathbf{y} || \\hat{\\mathbf{y}}_\\theta \\right) + \\lambda \\Vert\\theta\\Vert^2$\n",
        "\n",
        "A few things to note:\n",
        "\n",
        "- the loss incorporates $L_2$ regularization.\n",
        "- for STS, the KL divergence can be computed by considering the actual scores $\\mathbf{y}$ over all test instances as the first probability distribution, and the estimated scores $\\hat{\\mathbf{y}}_\\theta$ as the second distribution."
      ],
      "metadata": {
        "id": "kYmZlGHzwBUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# Write your training code here, as per the above description.\n",
        "\n",
        "from typing import List, Tuple, Dict, Union\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, ckpt_save_path, dataset, train_loader):\n",
        "        self.model = model\n",
        "        self.ckpt_save_path = ckpt_save_path\n",
        "        self.dataset = dataset\n",
        "        self.train_loader = train_loader\n",
        "\n",
        "    def train(self, dataset, max_epochs, ckpt_interval, validation_interval, device=\"cpu\", learning_rate=0.01, batch_size=8, loss_fn = 'kl'):\n",
        "        optimizer = torch.optim.Adam(params=self.model.parameters(), lr=learning_rate)\n",
        "\n",
        "        if loss_fn == 'kl':\n",
        "            loss_fn =nn.KLDivLoss(reduction='batchmean') #Use KLDivLoss by default need to be able to change though\n",
        "        else:\n",
        "            loss_fn =nn.MSELoss()\n",
        "\n",
        "        self.model.to(device)\n",
        "\n",
        "        progress = tqdm(range(max_epochs))\n",
        "        training_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        #Run though specified amount of epochs\n",
        "        for epoch in progress:\n",
        "            self.model.train()\n",
        "\n",
        "            total_loss = 0.0\n",
        "            for batch in self.train_loader:  #Need to extract batches\n",
        "                sentence1, sentence2, label = batch\n",
        "                label = label.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                output =self.model(sentence1, sentence2,self.dataset.word2doc_counts)  #forward pass\n",
        "                #print(output)\n",
        "                loss = loss_fn(output, label)\n",
        "\n",
        "                loss.backward()   #backward pass\n",
        "\n",
        "                #Check params and grads\n",
        "                # for name, param in self.model.named_parameters():\n",
        "                #     print(f'Gradient of {name}:')\n",
        "                #     print(param.grad)\n",
        "\n",
        "\n",
        "                optimizer.step()  #update the parameters\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                num_batches += 1\n",
        "                #print('loss:', loss)\n",
        "                #print('total_loss: ', total_loss)\n",
        "            training_loss = total_loss / num_batches\n",
        "            progress.set_description(\"Epoch %d - Average loss: %.4f\" % (epoch + 1, training_loss))\n",
        "\n",
        "            if (epoch> 0) and (epoch % ckpt_interval == 0) :\n",
        "                self.save_model_checkpoint(epoch)\n",
        "\n",
        "    def save_model_checkpoint(self, current_epoch):\n",
        "        torch.save(self.model.state_dict(), \"%s/%s.ckpt\" % (self.ckpt_save_path, str(current_epoch)))\n"
      ],
      "metadata": {
        "id": "7W_P4FqEyPiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "def create_path(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.mkdir(path)\n",
        "        print (\"Created the path: %s\" % (path))\n",
        "\n",
        "def run_training(dataset, #dataset of all sets\n",
        "                 loss_fn='kl', #loss function to use\n",
        "                 embedding_dimensions=50,#pretrained emebding dimensions to use\n",
        "                 regularization_parameter=0.4, #regularization param to use\n",
        "                 window_sizes = [2,3,4], #window sizes to use\n",
        "                 convolution_filters_used=6, #number of filters to use\n",
        "                 similarity_fn='cosine', #similarity function to use\n",
        "                 ckpt_model_path='./ckpt', #path to checkpoint\n",
        "                 final_model_path='./final_model_ckpt', # path to trained model\n",
        "                 batch_size=20,#batch size\n",
        "                 ckpt_epoch_size=2, #number of epochs after model is saved\n",
        "                 epochs=20): #number of epochs\n",
        "\n",
        "    ckpt_model_path = f'{ckpt_model_path}_glove.{embedding_dimensions}d.tuned.{convolution_filters_used}-filters.{loss_fn}-loss'\n",
        "    create_path(ckpt_model_path)\n",
        "\n",
        "\n",
        "    vocab_size = len(dataset.word2doc_counts)\n",
        "\n",
        "    train_data, dev_data, test_data = dataset.splits()\n",
        "\n",
        "    #Find padding length for all sentences by finding max length and adding 5, just chose 5 may need to change\n",
        "    longest_sentence = 0\n",
        "    for sentence_1, sentence_2,  in zip(train_data['sentence_1'], train_data['sentence_2']):\n",
        "        max_length = max(len(sentence_1.split()), len(sentence_2.split()))\n",
        "        if max_length > longest_sentence:\n",
        "            longest_sentence =max_length\n",
        "\n",
        "    padding_length = longest_sentence + 5\n",
        "\n",
        "    train_dataset = []\n",
        "    for sen_1, sen_2, label in zip(train_data['sentence_1'], train_data['sentence_2'], train_data['label']):\n",
        "        train_dataset.append((sen_1, sen_2, label))\n",
        "\n",
        "    #print(train_dataset)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    #Get word to counts\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = ConvolutionalTwinNetwork(embedding_dim=embedding_dimensions,\n",
        "                                     vocab_size=vocab_size,\n",
        "                                     filter_widths=window_sizes,\n",
        "                                     num_filters=convolution_filters_used,\n",
        "                                     hidden_dim=128,\n",
        "                                     similarity_measurement=similarity_fn,\n",
        "                                     convolution_layer='our_conv',\n",
        "                                     padding_size=padding_length,\n",
        "                                     device = device\n",
        "                                     )\n",
        "\n",
        "    trainer = Trainer(model, ckpt_model_path,dataset, train_loader)\n",
        "\n",
        "    this_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f'Device: {this_device}')\n",
        "\n",
        "    trainer.train(dataset,\n",
        "                  max_epochs=epochs,\n",
        "                  ckpt_interval=ckpt_epoch_size,\n",
        "                  validation_interval=1000,\n",
        "                  device=this_device,\n",
        "                  batch_size=batch_size,\n",
        "                  loss_fn = loss_fn)\n",
        "\n",
        "    create_path(final_model_path)\n",
        "    model_filepath = os.path.join(final_model_path, f'glove.{embedding_dimensions}d.tuned.{convolution_filters_used}-filters.{loss_fn}-loss.pkl')\n",
        "    property_filepath = os.path.join(final_model_path, f'glove.{embedding_dimensions}d.tuned.{convolution_filters_used}-filters.{loss_fn}-loss-properties.md')\n",
        "    with open(model_filepath, 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "\n",
        "    property_dict = {\n",
        "        'Embedding_dimension': embedding_dimensions,\n",
        "        'Training_loss_function' :loss_fn,\n",
        "        'Regularization_parameter_(in_loss_function)':regularization_parameter,\n",
        "        'Window_sizes_used': window_sizes,\n",
        "        'Number_of_convolutional_filters_used' : convolution_filters_used,\n",
        "        'Similarity_computation_(cosine/structured)': similarity_fn,\n",
        "        'Number_of_training_epochs': epochs,\n",
        "        'Batch_size_used_for_training' : batch_size\n",
        "    }\n",
        "    with open(property_filepath, 'wb') as f:\n",
        "        pickle.dump(property_dict,f)"
      ],
      "metadata": {
        "id": "KUP2c40wh_s9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test with dev\n",
        "dataset = SickDataset(\"/content/sick_dataset\")\n",
        "run_training(dataset, epochs=5, embedding_dimensions=50, convolution_filters_used=3, similarity_fn='cosine', ckpt_model_path='./dev_ckpt', final_model_path='./dev_ckpt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Gnr9oFX1Yrq",
        "outputId": "488dcfaf-253d-4d41-f9b1-8330defe2d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModuleList(\n",
            "  (0-2): 3 x ConvolutionalFilter(\n",
            "    (activation_function): Tanh()\n",
            "  )\n",
            ")\n",
            "Device: cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - Average loss: -1.6579: 100%|██████████| 5/5 [02:55<00:00, 35.16s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test with cosine\n",
        "dataset = SickDataset(\"/content/sick_dataset\")\n",
        "run_training(dataset, epochs=25, embedding_dimensions=200, convolution_filters_used=18, similarity_fn='cosine', ckpt_model_path='./cosine_ckpt', final_model_path='./final_model_cosine_ckpt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPpAFhKZUNXq",
        "outputId": "a41a6745-2eda-4c95-9db3-4e47705a32bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created the path: ./cosine_ckpt_glove.200d.tuned.18-filters.kl-loss\n",
            "ModuleList(\n",
            "  (0-17): 18 x ConvolutionalFilter(\n",
            "    (activation_function): Tanh()\n",
            "  )\n",
            ")\n",
            "Device: cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25 - Average loss: -0.3319: 100%|██████████| 25/25 [1:05:26<00:00, 157.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created the path: ./final_model_cosine_ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test with structured similarity function 1\n",
        "dataset = SickDataset(\"/content/sick_dataset\")\n",
        "run_training(dataset, epochs=15, embedding_dimensions=200, convolution_filters_used=18, similarity_fn='structure', ckpt_model_path='./structure_ckpt', final_model_path='./final_model_structure_ckpt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eX7C8JU_vKtK",
        "outputId": "611db736-eeaf-47a3-b691-917a844e1d59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModuleList(\n",
            "  (0-17): 18 x ConvolutionalFilter(\n",
            "    (activation_function): Tanh()\n",
            "  )\n",
            ")\n",
            "Device: cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15 - Average loss: -11.0492: 100%|██████████| 15/15 [20:29<00:00, 81.99s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created the path: ./final_model_structure_ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test with structured similarity function 2\n",
        "dataset = SickDataset(\"/content/sick_dataset\")\n",
        "run_training(dataset, epochs=12, embedding_dimensions=300, convolution_filters_used=12, similarity_fn='structure', ckpt_model_path='./structure2_ckpt', final_model_path='./final_model_structure2_ckpt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrdvhOIJ0lrs",
        "outputId": "f94fc22c-108a-4ff2-93b2-e45617d445fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModuleList(\n",
            "  (0-11): 12 x ConvolutionalFilter(\n",
            "    (activation_function): Tanh()\n",
            "  )\n",
            ")\n",
            "Device: cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12 - Average loss: -13.8110: 100%|██████████| 12/12 [12:04<00:00, 60.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created the path: ./final_model_structure2_ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test with best KL loss\n",
        "dataset = SickDataset(\"/content/sick_dataset\")\n",
        "run_training(dataset, epochs=12, embedding_dimensions=300, convolution_filters_used=24, similarity_fn='cosine', ckpt_model_path='./best_kl_ckpt', final_model_path='./final_model_best_kl_ckpt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNAYKnye0sc9",
        "outputId": "ea08c4e1-0351-4d12-a64e-8ec30b19e93e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created the path: ./best_kl_ckpt_glove.300d.tuned.24-filters.kl-loss\n",
            "ModuleList(\n",
            "  (0-23): 24 x ConvolutionalFilter(\n",
            "    (activation_function): Tanh()\n",
            "  )\n",
            ")\n",
            "Device: cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12 - Average loss: -0.6906: 100%|██████████| 12/12 [46:04<00:00, 230.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created the path: ./final_model_best_kl_ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset = SickDataset(\"/content/sick_dataset\")\n",
        "\n",
        "# vocab_size = len(dataset.word2doc_counts)\n",
        "\n",
        "# train_data, dev_data, test_data = dataset.splits()\n",
        "\n",
        "# print(train_data)"
      ],
      "metadata": {
        "id": "HgIwVmSZdq5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.7 Experiments and Evaluation\n",
        "\n",
        "In this final leg of the assignment, you are required to report on your experiments and the final results. Unlike the previous assignments, the evaluation here is not in terms of precision, recall, and accuracy. Rather, it is in terms of the following:\n",
        "\n",
        "- Pearson's correlation coefficient $r$\n",
        "- Mean Squared Error (MSE)\n",
        "\n",
        "Your aim should be to achieve $r \\geq 0.80$ and MSE $\\leq 0.35$ on the test set. This is a somewhat ambitious aim, so please don't despair if you can't reach this goal! These numbers are give to you as a yardstick.\n",
        "\n",
        "When you finish fine-tuning and start to achieve decent results, you should save your model as a `.zip` (see the last section on what to submit).\n",
        "\n",
        "For grading the performance of your models, four models are required:\n",
        "\n",
        "- Your two best models using structured similarity computation\n",
        "- Your best model using cosine similarity\n",
        "- Your best model among those trained using KL divergence loss. If this criterion is already fulfilled by the earlier models, then you should provide your best fine-tuned model that was trained using mean squared error as the loss function.\n",
        "\n"
      ],
      "metadata": {
        "id": "PNu_67c1ybyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr #Used to calculate corr\n",
        "\n",
        "def load_finetuned_embeddings(embeddings_path):\n",
        "    \"\"\"\n",
        "    Load fine-tuned GloVe embeddings from a file.\n",
        "    \"\"\"\n",
        "    # Load embeddings from file\n",
        "    # Assuming embeddings are stored as numpy arrays\n",
        "    embeddings = np.load(embeddings_path, allow_pickle=True)\n",
        "    return embeddings\n",
        "\n",
        "def load_model_properties(properties_path):\n",
        "    \"\"\"\n",
        "    Load model properties from a file.\n",
        "    \"\"\"\n",
        "    #Load properties from file\n",
        "    properties = np.load(properties_path, allow_pickle=True)\n",
        "    return properties\n",
        "\n",
        "def evaluate_model_metrics(model, test_loader, loss_fn, device):\n",
        "    \"\"\"\n",
        "    Calculates both the correlation and MSE given our model, dataloader and device.\n",
        "    \"\"\"\n",
        "    targets = []\n",
        "    predictions = []\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    #Use testing modes dont want to update model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            sentence1,sentence2,label = batch\n",
        "            label = label.to(device)\n",
        "\n",
        "            output =model(sentence1,sentence2,dataset.word2doc_counts)\n",
        "            # print('output', output)\n",
        "            # print('label', label)\n",
        "\n",
        "            #Store for MSE\n",
        "            loss =loss_fn(output, label)  # Assuming your output needs log_softmax\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            #Store for corr\n",
        "            predictions.extend(output.cpu().numpy() )\n",
        "            targets.extend(label.cpu().numpy())\n",
        "\n",
        "    # print(predictions)\n",
        "    # print(targets)\n",
        "\n",
        "    #Calc corr\n",
        "    predictions = np.array(predictions).squeeze()\n",
        "    correlation, _= pearsonr(predictions, targets)\n",
        "\n",
        "    #Calc MSE\n",
        "    mse=np.mean(losses)\n",
        "\n",
        "    return (correlation, mse)\n",
        "\n",
        "def test(finetuned_embeddings_path, dataset,  model_properties_file_path=None) -> None:\n",
        "    \"\"\"\n",
        "    Computes the Pearson's correlation coefficient r and the mean squared error (MSE) for fine-tuned GloVe embeddings\n",
        "    and prints the result in a neat tabular structure with model details, as follows (actual values not displayed):\n",
        "\n",
        "    Embedding dimension                            200\n",
        "    Training loss function                         KL Divergence Loss\n",
        "    Regularization parameter (in loss function)    0.4\n",
        "    Window sizes used                              2, 3, 4\n",
        "    Number of convolutional filters used           15\n",
        "    Number of filters with max pooling             8\n",
        "    Number of filters with min pooling             5\n",
        "    Number of filters with mean pooling            2\n",
        "    Similarity computation (cosine/structured)     structured\n",
        "    Number of training epochs                      8\n",
        "    Batch size used for training                   20\n",
        "\n",
        "    Pearson's Correlation Coefficient              0.821\n",
        "    Mean Squared Error                             0.344\n",
        "    \"\"\"\n",
        "\n",
        "    #Get model and properties\n",
        "    model = load_finetuned_embeddings(finetuned_embeddings_path)\n",
        "    properties = load_model_properties(model_properties_file_path)\n",
        "\n",
        "\n",
        "    #Print properties\n",
        "    for key, value in properties.items():\n",
        "        key = key.replace(\"_\", \" \")\n",
        "        print(key, \": \", value)\n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    train_data, dev_data, test_data = dataset.splits()\n",
        "\n",
        "\n",
        "    test_dataset = []\n",
        "    for sen_1, sen_2, label in zip(test_data['sentence_1'], test_data['sentence_2'], test_data['label']):\n",
        "        test_dataset.append((sen_1, sen_2, label))\n",
        "\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    #Get MSE and corr using helper function\n",
        "    correlation, mse = evaluate_model_metrics(model, test_loader, loss_fn, device)\n",
        "\n",
        "    #Print MSE and corr\n",
        "    print(f\"Pearson's Correlation Coefficient: {correlation:0.4f}\")\n",
        "    print(f\"Mean Squared Error: {mse:0.4f}\")\n",
        "\n",
        "def worst(finetuned_embeddings_path, dataset,  model_properties_file_path=None, k=10):\n",
        "    \"\"\"\n",
        "    Returns the top k sentence pairs for which your model's estimated score was the worst (by worst, we mean the\n",
        "    highest difference between the gold-standard relatedness score and your predicted score).\n",
        "    \"\"\"\n",
        "\n",
        "    #Get model and properties\n",
        "    model = load_finetuned_embeddings(finetuned_embeddings_path)\n",
        "    properties = load_model_properties(model_properties_file_path)\n",
        "\n",
        "    #Get test data\n",
        "    train_data, dev_data, test_data = dataset.splits()\n",
        "\n",
        "    test_dataset = []\n",
        "    for sen_1, sen_2, label in zip(test_data['sentence_1'], test_data['sentence_2'], test_data['label']):\n",
        "        test_dataset.append((sen_1, sen_2, label))\n",
        "\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    #Loss fn is now MSE\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    differences = []\n",
        "\n",
        "    #Use testing modes dont want to update model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            sentence1, sentence2, label = batch\n",
        "            label = label.to(device)\n",
        "\n",
        "            output = model(sentence1, sentence2, dataset.word2doc_counts)\n",
        "\n",
        "            #Get difference between our outputs and labels (true resutls)\n",
        "            difference = torch.abs(output - label)\n",
        "            differences.append((sentence1, sentence2, difference.item(), output, label))\n",
        "\n",
        "    #Sort list by worst difference\n",
        "    worst_pairs = sorted(differences, key=lambda x: x[2], reverse=True)\n",
        "    worst_pairs_k = worst_pairs[:k]\n",
        "\n",
        "    #Print k worst with stats\n",
        "    for i, (sentence1, sentence2, difference, output, label) in enumerate(worst_pairs_k):\n",
        "        print()\n",
        "        print(f\"Worst Pair {i + 1}:\")\n",
        "        print(\"Sentence 1: \", sentence1)\n",
        "        print(\"Sentence 2: \", sentence2)\n",
        "        print(\"Difference: \", difference)\n",
        "        print(\"Model Output: \", output)\n",
        "        print(\"True Label: \", label)\n",
        "        print()\n",
        "\n",
        "def best(finetuned_embeddings_path, dataset,  model_properties_file_path=None, k=10):\n",
        "    \"\"\"\n",
        "    Returns the top k sentence pairs for which your model's estimated score was the best (by best, we mean that the\n",
        "    difference between the gold-standard relatedness score and your predicted score was the lowest)\n",
        "    \"\"\"\n",
        "\n",
        "    #Get model and properties\n",
        "    model = load_finetuned_embeddings(finetuned_embeddings_path)\n",
        "    properties = load_model_properties(model_properties_file_path)\n",
        "\n",
        "    #Get test data\n",
        "    train_data, dev_data, test_data = dataset.splits()\n",
        "\n",
        "    test_dataset = []\n",
        "    for sen_1, sen_2, label in zip(test_data['sentence_1'], test_data['sentence_2'], test_data['label']):\n",
        "        test_dataset.append((sen_1, sen_2, label))\n",
        "\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    #Loss fn is now MSE\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    differences = []\n",
        "\n",
        "    #Use testing modes dont want to update model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            sentence1, sentence2, label = batch\n",
        "            label = label.to(device)\n",
        "\n",
        "            output = model(sentence1, sentence2, dataset.word2doc_counts)\n",
        "\n",
        "            #Get difference between our outputs and labels (true resutls)\n",
        "            difference = torch.abs(output - label)\n",
        "            differences.append((sentence1, sentence2, difference.item(), output, label))\n",
        "\n",
        "    #Sort list by best difference (closest)\n",
        "    best_pairs = sorted(differences, key=lambda x: x[2], reverse=False)\n",
        "    best_pairs_k = best_pairs[:k]\n",
        "\n",
        "    #Print k best with stats\n",
        "    for i, (sentence1, sentence2, difference, output, label) in enumerate(best_pairs_k):\n",
        "        print()\n",
        "        print(f\"Best Pair {i + 1}:\")\n",
        "        print(\"Sentence 1: \", sentence1)\n",
        "        print(\"Sentence 2: \", sentence2)\n",
        "        print(\"Difference: \", difference)\n",
        "        print(\"Model Output: \", output)\n",
        "        print(\"True Label: \", label)\n",
        "        print()"
      ],
      "metadata": {
        "id": "hu-sTfqG5STq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test for cosine model with 200 embed, 18 filters, and kl loss\n",
        "test('/content/final_model_cosine_ckpt/glove.200d.tuned.18-filters.kl-loss.pkl',dataset,'/content/final_model_cosine_ckpt/glove.200d.tuned.18-filters.kl-loss-properties.md')\n",
        "worst('/content/final_model_cosine_ckpt/glove.200d.tuned.18-filters.kl-loss.pkl',dataset,'/content/final_model_cosine_ckpt/glove.200d.tuned.18-filters.kl-loss-properties.md')\n",
        "best('/content/final_model_cosine_ckpt/glove.200d.tuned.18-filters.kl-loss.pkl',dataset,'/content/final_model_cosine_ckpt/glove.200d.tuned.18-filters.kl-loss-properties.md')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7yhTGx6wPT_",
        "outputId": "9084c1ab-6535-4fd9-b24a-50f5fe6cd4af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding dimension :  200\n",
            "Training loss function :  kl\n",
            "Regularization parameter (in loss function) :  0.4\n",
            "Window sizes used :  [2, 3, 4]\n",
            "Number of convolutional filters used :  18\n",
            "Similarity computation (cosine/structured) :  cosine\n",
            "Number of training epochs :  25\n",
            "Batch size used for training :  20\n",
            "Pearson's Correlation Coefficient: 0.0995\n",
            "Mean Squared Error: 1.0460\n",
            "\n",
            "Worst Pair 1:\n",
            "Sentence 1:  ('The band is singing ',)\n",
            "Sentence 2:  ('A woman is carefully removing her makeup',)\n",
            "Difference:  2.666499614715576\n",
            "Model Output:  tensor([3.6665], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 2:\n",
            "Sentence 1:  ('A jet is not flying',)\n",
            "Sentence 2:  ('A dog is barking',)\n",
            "Difference:  2.6664578914642334\n",
            "Model Output:  tensor([3.6665], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 3:\n",
            "Sentence 1:  ('A woman is putting on makeup carefully',)\n",
            "Sentence 2:  ('The band is singing ',)\n",
            "Difference:  2.666428565979004\n",
            "Model Output:  tensor([3.6664], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 4:\n",
            "Sentence 1:  ('A man is jumping rope in a field',)\n",
            "Sentence 2:  ('A woman is slicing a cucumber',)\n",
            "Difference:  2.6663894653320312\n",
            "Model Output:  tensor([3.6664], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 5:\n",
            "Sentence 1:  ('A baby elephant is eating a small tree',)\n",
            "Sentence 2:  ('A little girl is selling a scooter',)\n",
            "Difference:  2.6663877964019775\n",
            "Model Output:  tensor([3.6664], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 6:\n",
            "Sentence 1:  ('A white horse is standing',)\n",
            "Sentence 2:  ('A woman is cutting an onion',)\n",
            "Difference:  2.6663646697998047\n",
            "Model Output:  tensor([3.6664], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 7:\n",
            "Sentence 1:  ('A cat is jumping into a box',)\n",
            "Sentence 2:  ('A young man is talking to a leaf',)\n",
            "Difference:  2.666348934173584\n",
            "Model Output:  tensor([3.6663], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 8:\n",
            "Sentence 1:  ('A woman is putting on some makeup carefully,',)\n",
            "Sentence 2:  ('The band is singing ',)\n",
            "Difference:  2.6663458347320557\n",
            "Model Output:  tensor([3.6663], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 9:\n",
            "Sentence 1:  ('A man is cutting a lemon',)\n",
            "Sentence 2:  ('A woman is firing a rifle',)\n",
            "Difference:  2.6663355827331543\n",
            "Model Output:  tensor([3.6663], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 10:\n",
            "Sentence 1:  ('A baby elephant is not eating a small tree',)\n",
            "Sentence 2:  ('A little girl is peddling a scooter',)\n",
            "Difference:  2.666330337524414\n",
            "Model Output:  tensor([3.6663], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 1:\n",
            "Sentence 1:  ('A man is holding a small animal in one hand',)\n",
            "Sentence 2:  ('A man is exhibiting a small monkey',)\n",
            "Difference:  0.0008635139465331676\n",
            "Model Output:  tensor([3.6659], device='cuda:0')\n",
            "True Label:  tensor([3.6650], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 2:\n",
            "Sentence 1:  ('Some horses are racing furiously over the green grass and jumping an obstacle',)\n",
            "Sentence 2:  ('The Jockeys are racing horses on the field, which is completely green',)\n",
            "Difference:  0.011718893051147372\n",
            "Model Output:  tensor([3.6117], device='cuda:0')\n",
            "True Label:  tensor([3.6000], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 3:\n",
            "Sentence 1:  ('A man on inline skates is skating at a skate park',)\n",
            "Sentence 2:  ('A man on inline skates is resting at a skate park',)\n",
            "Difference:  0.01661524772644052\n",
            "Model Output:  tensor([3.6666], device='cuda:0')\n",
            "True Label:  tensor([3.6500], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 4:\n",
            "Sentence 1:  ('People are looking at some costumes gathered in the vicinity of the forest',)\n",
            "Sentence 2:  ('People wearing costumes are gathering in a forest and are looking in the same direction',)\n",
            "Difference:  0.026255121231079315\n",
            "Model Output:  tensor([3.6613], device='cuda:0')\n",
            "True Label:  tensor([3.6350], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 5:\n",
            "Sentence 1:  ('Two women dressed in white and black are sitting on a bench',)\n",
            "Sentence 2:  ('Two people are sitting on a park bench on a sunny day',)\n",
            "Difference:  0.03129791259765646\n",
            "Model Output:  tensor([3.6663], device='cuda:0')\n",
            "True Label:  tensor([3.6350], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 6:\n",
            "Sentence 1:  ('A woman is playing the flute',)\n",
            "Sentence 2:  ('A man is playing the flute',)\n",
            "Difference:  0.03333325386047381\n",
            "Model Output:  tensor([3.6667], device='cuda:0')\n",
            "True Label:  tensor([3.7000], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 7:\n",
            "Sentence 1:  ('No young child is riding a three wheeled scooter down the sidewalk',)\n",
            "Sentence 2:  ('A young child is riding a three wheeled scooter down the sidewalk',)\n",
            "Difference:  0.03333325386047381\n",
            "Model Output:  tensor([3.6667], device='cuda:0')\n",
            "True Label:  tensor([3.7000], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 8:\n",
            "Sentence 1:  ('One white dog and one black one are sitting side by side on the grass',)\n",
            "Sentence 2:  ('One white dog and one black one are running side by side on the grass',)\n",
            "Difference:  0.03333325386047381\n",
            "Model Output:  tensor([3.6667], device='cuda:0')\n",
            "True Label:  tensor([3.7000], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 9:\n",
            "Sentence 1:  ('A woman is standing at dusk on an almost pristine, snowy road, that is lit only by headlights',)\n",
            "Sentence 2:  ('A man is standing at dusk on an almost pristine, snowy road, that is lit only by headlights',)\n",
            "Difference:  0.03333325386047381\n",
            "Model Output:  tensor([3.6667], device='cuda:0')\n",
            "True Label:  tensor([3.7000], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 10:\n",
            "Sentence 1:  ('Two young girl are looking up at the camera and one is sticking out his tongue',)\n",
            "Sentence 2:  ('Two young boys are looking up at the camera and one is sticking out his tongue',)\n",
            "Difference:  0.03333325386047381\n",
            "Model Output:  tensor([3.6667], device='cuda:0')\n",
            "True Label:  tensor([3.7000], device='cuda:0', dtype=torch.float64)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Structured 1 model results\n",
        "test('/content/final_model_structure_ckpt/glove.200d.tuned.18-filters.kl-loss.pkl',dataset,'/content/final_model_structure_ckpt/glove.200d.tuned.18-filters.kl-loss-properties.md')\n",
        "worst('/content/final_model_structure_ckpt/glove.200d.tuned.18-filters.kl-loss.pkl',dataset,'/content/final_model_structure_ckpt/glove.200d.tuned.18-filters.kl-loss-properties.md')\n",
        "best('/content/final_model_structure_ckpt/glove.200d.tuned.18-filters.kl-loss.pkl',dataset,'/content/final_model_structure_ckpt/glove.200d.tuned.18-filters.kl-loss-properties.md')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaflFXNAGo_U",
        "outputId": "58c00391-1df9-4533-b5e8-eac8b2950b82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding dimension :  200\n",
            "Training loss function :  kl\n",
            "Regularization parameter (in loss function) :  0.4\n",
            "Window sizes used :  [2, 3, 4]\n",
            "Number of convolutional filters used :  18\n",
            "Similarity computation (cosine/structured) :  structure\n",
            "Number of training epochs :  15\n",
            "Batch size used for training :  20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pearson's Correlation Coefficient: 0.4235\n",
            "Mean Squared Error: 1.0475\n",
            "\n",
            "Worst Pair 1:\n",
            "Sentence 1:  ('The dog is catching a ball',)\n",
            "Sentence 2:  ('A small girl is riding in a toy car',)\n",
            "Difference:  2.666635036468506\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 2:\n",
            "Sentence 1:  ('A man is shooting guns',)\n",
            "Sentence 2:  ('A woman is not riding a horse',)\n",
            "Difference:  2.6666347980499268\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 3:\n",
            "Sentence 1:  ('A dog is sitting on the ground',)\n",
            "Sentence 2:  ('A girl is tapping her fingernails',)\n",
            "Difference:  2.6666340827941895\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 4:\n",
            "Sentence 1:  ('Two kids are doing martial arts on a blue mat',)\n",
            "Sentence 2:  ('A tan dog is running through the brush',)\n",
            "Difference:  2.6666276454925537\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 5:\n",
            "Sentence 1:  ('A man is shooting a shotgun',)\n",
            "Sentence 2:  ('Someone is playing the guitar',)\n",
            "Difference:  2.6666266918182373\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 6:\n",
            "Sentence 1:  ('A man is playing the guitar',)\n",
            "Sentence 2:  ('A woman is frying something in a pan',)\n",
            "Difference:  2.6666266918182373\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 7:\n",
            "Sentence 1:  ('A baby elephant is eating a small tree',)\n",
            "Sentence 2:  ('A little girl is selling a scooter',)\n",
            "Difference:  2.6666259765625\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 8:\n",
            "Sentence 1:  ('A woman is dipping a shrimp in batter',)\n",
            "Sentence 2:  ('There is no man driving the car',)\n",
            "Difference:  2.6666245460510254\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 9:\n",
            "Sentence 1:  ('The woman is pouring oil into the pan',)\n",
            "Sentence 2:  ('The boy is playing the piano',)\n",
            "Difference:  2.666623592376709\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 10:\n",
            "Sentence 1:  ('A white horse is standing',)\n",
            "Sentence 2:  ('A woman is cutting an onion',)\n",
            "Difference:  2.666623592376709\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 1:\n",
            "Sentence 1:  ('A man is holding a small animal in one hand',)\n",
            "Sentence 2:  ('A man is exhibiting a small monkey',)\n",
            "Difference:  0.0016078567504882457\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([3.6650], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 2:\n",
            "Sentence 1:  ('A man on inline skates is skating at a skate park',)\n",
            "Sentence 2:  ('A man on inline skates is resting at a skate park',)\n",
            "Difference:  0.016647911071777433\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([3.6500], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 3:\n",
            "Sentence 1:  ('People are looking at some costumes gathered in the vicinity of the forest',)\n",
            "Sentence 2:  ('People wearing costumes are gathering in a forest and are looking in the same direction',)\n",
            "Difference:  0.03162096977233908\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([3.6350], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 4:\n",
            "Sentence 1:  ('Two women dressed in white and black are sitting on a bench',)\n",
            "Sentence 2:  ('Two people are sitting on a park bench on a sunny day',)\n",
            "Difference:  0.0316240692138674\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([3.6350], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 5:\n",
            "Sentence 1:  ('A woman is standing at dusk on an almost pristine, snowy road, that is lit only by headlights',)\n",
            "Sentence 2:  ('A man is standing at dusk on an almost pristine, snowy road, that is lit only by headlights',)\n",
            "Difference:  0.03334398269653338\n",
            "Model Output:  tensor([[3.6667]], device='cuda:0')\n",
            "True Label:  tensor([3.7000], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 6:\n",
            "Sentence 1:  ('A cow is not eating hay',)\n",
            "Sentence 2:  ('A cow is eating hay',)\n",
            "Difference:  0.03334851264953631\n",
            "Model Output:  tensor([[3.6667]], device='cuda:0')\n",
            "True Label:  tensor([3.7000], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 7:\n",
            "Sentence 1:  ('A parrot is speaking',)\n",
            "Sentence 2:  ('There is no parrot speaking',)\n",
            "Difference:  0.03334946632385272\n",
            "Model Output:  tensor([[3.6667]], device='cuda:0')\n",
            "True Label:  tensor([3.7000], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 8:\n",
            "Sentence 1:  ('The man is not talking on the phone',)\n",
            "Sentence 2:  ('The man is talking on the telephone',)\n",
            "Difference:  0.03334970474243182\n",
            "Model Output:  tensor([[3.6667]], device='cuda:0')\n",
            "True Label:  tensor([3.7000], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 9:\n",
            "Sentence 1:  ('A tan dog is resting in the snow',)\n",
            "Sentence 2:  ('A tan dog is running through the snow',)\n",
            "Difference:  0.03335161209106463\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([3.7000], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 10:\n",
            "Sentence 1:  ('A dog is sleeping on the green grass',)\n",
            "Sentence 2:  ('A dog is playing on the green grass',)\n",
            "Difference:  0.03335256576538104\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([3.7000], device='cuda:0', dtype=torch.float64)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Structured 2 model results\n",
        "test('/content/final_model_structure2_ckpt/glove.300d.tuned.12-filters.kl-loss.pkl',dataset,'/content/final_model_structure2_ckpt/glove.300d.tuned.12-filters.kl-loss-properties.md')\n",
        "worst('/content/final_model_structure2_ckpt/glove.300d.tuned.12-filters.kl-loss.pkl',dataset,'/content/final_model_structure2_ckpt/glove.300d.tuned.12-filters.kl-loss-properties.md')\n",
        "best('/content/final_model_structure2_ckpt/glove.300d.tuned.12-filters.kl-loss.pkl',dataset,'/content/final_model_structure2_ckpt/glove.300d.tuned.12-filters.kl-loss-properties.md')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nQt_AqNG9YE",
        "outputId": "66689b44-2cb7-438b-cfed-a7cb4eba24c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding dimension :  300\n",
            "Training loss function :  kl\n",
            "Regularization parameter (in loss function) :  0.4\n",
            "Window sizes used :  [2, 3, 4]\n",
            "Number of convolutional filters used :  12\n",
            "Similarity computation (cosine/structured) :  structure\n",
            "Number of training epochs :  12\n",
            "Batch size used for training :  20\n",
            "Pearson's Correlation Coefficient: 0.4248\n",
            "Mean Squared Error: 1.0475\n",
            "\n",
            "Worst Pair 1:\n",
            "Sentence 1:  ('An email is being read by a man',)\n",
            "Sentence 2:  ('A person is peeling a banana',)\n",
            "Difference:  2.666576862335205\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 2:\n",
            "Sentence 1:  ('A baby elephant is eating a small tree',)\n",
            "Sentence 2:  ('A little girl is selling a scooter',)\n",
            "Difference:  2.666566848754883\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 3:\n",
            "Sentence 1:  ('A baby elephant is not eating a small tree',)\n",
            "Sentence 2:  ('A little girl is peddling a scooter',)\n",
            "Difference:  2.666565418243408\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 4:\n",
            "Sentence 1:  ('A man is shooting guns',)\n",
            "Sentence 2:  ('A woman is not riding a horse',)\n",
            "Difference:  2.666560173034668\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 5:\n",
            "Sentence 1:  ('A dog is sitting on the ground',)\n",
            "Sentence 2:  ('A girl is tapping her fingernails',)\n",
            "Difference:  2.666558265686035\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 6:\n",
            "Sentence 1:  ('A woman is cutting a white onion',)\n",
            "Sentence 2:  ('A horse is standing',)\n",
            "Difference:  2.666555881500244\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 7:\n",
            "Sentence 1:  ('The dog is catching a ball',)\n",
            "Sentence 2:  ('A small girl is riding in a toy car',)\n",
            "Difference:  2.6665544509887695\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 8:\n",
            "Sentence 1:  ('An elephant is being ridden by a woman',)\n",
            "Sentence 2:  ('A woman is opening a soda and drinking it',)\n",
            "Difference:  2.6665542125701904\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 9:\n",
            "Sentence 1:  ('The people are walking on the road beside a beautiful waterfall',)\n",
            "Sentence 2:  ('There is no brown dog and black dog playing in the sand',)\n",
            "Difference:  2.666551351547241\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 10:\n",
            "Sentence 1:  ('Two kids are doing martial arts on a blue mat',)\n",
            "Sentence 2:  ('A tan dog is running through the brush',)\n",
            "Difference:  2.666551351547241\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 1:\n",
            "Sentence 1:  ('A man is holding a small animal in one hand',)\n",
            "Sentence 2:  ('A man is exhibiting a small monkey',)\n",
            "Difference:  0.0015420532226562145\n",
            "Model Output:  tensor([[3.6665]], device='cuda:0')\n",
            "True Label:  tensor([3.6650], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 2:\n",
            "Sentence 1:  ('A man on inline skates is skating at a skate park',)\n",
            "Sentence 2:  ('A man on inline skates is resting at a skate park',)\n",
            "Difference:  0.016608333587646573\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([3.6500], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 3:\n",
            "Sentence 1:  ('People are looking at some costumes gathered in the vicinity of the forest',)\n",
            "Sentence 2:  ('People wearing costumes are gathering in a forest and are looking in the same direction',)\n",
            "Difference:  0.031551351547241424\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([3.6350], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 4:\n",
            "Sentence 1:  ('Two women dressed in white and black are sitting on a bench',)\n",
            "Sentence 2:  ('Two people are sitting on a park bench on a sunny day',)\n",
            "Difference:  0.03155850410461447\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([3.6350], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 5:\n",
            "Sentence 1:  ('A woman is standing at dusk on an almost pristine, snowy road, that is lit only by headlights',)\n",
            "Sentence 2:  ('A man is standing at dusk on an almost pristine, snowy road, that is lit only by headlights',)\n",
            "Difference:  0.03337020874023455\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([3.7000], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 6:\n",
            "Sentence 1:  ('A cow is not eating hay',)\n",
            "Sentence 2:  ('A cow is eating hay',)\n",
            "Difference:  0.03338975906372088\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([3.7000], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 7:\n",
            "Sentence 1:  ('A parrot is speaking',)\n",
            "Sentence 2:  ('There is no parrot speaking',)\n",
            "Difference:  0.033390474319458185\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([3.7000], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 8:\n",
            "Sentence 1:  ('The man is not talking on the phone',)\n",
            "Sentence 2:  ('The man is talking on the telephone',)\n",
            "Difference:  0.033391904830932795\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([3.7000], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 9:\n",
            "Sentence 1:  ('A dog is sleeping on the green grass',)\n",
            "Sentence 2:  ('A dog is playing on the green grass',)\n",
            "Difference:  0.03339595794677752\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([3.7000], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 10:\n",
            "Sentence 1:  ('A tan dog is resting in the snow',)\n",
            "Sentence 2:  ('A tan dog is running through the snow',)\n",
            "Difference:  0.03339619636535662\n",
            "Model Output:  tensor([[3.6666]], device='cuda:0')\n",
            "True Label:  tensor([3.7000], device='cuda:0', dtype=torch.float64)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Best KL loss model results\n",
        "test('/content/final_model_best_kl_ckpt/glove.300d.tuned.24-filters.kl-loss.pkl',dataset,'/content/final_model_best_kl_ckpt/glove.300d.tuned.24-filters.kl-loss-properties.md')\n",
        "worst('/content/final_model_best_kl_ckpt/glove.300d.tuned.24-filters.kl-loss.pkl',dataset,'/content/final_model_best_kl_ckpt/glove.300d.tuned.24-filters.kl-loss-properties.md')\n",
        "best('/content/final_model_best_kl_ckpt/glove.300d.tuned.24-filters.kl-loss.pkl',dataset,'/content/final_model_best_kl_ckpt/glove.300d.tuned.24-filters.kl-loss-properties.md')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtNWgm5_H7XH",
        "outputId": "4427c507-146d-437f-d755-273b55ddf37a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding dimension :  300\n",
            "Training loss function :  kl\n",
            "Regularization parameter (in loss function) :  0.4\n",
            "Window sizes used :  [2, 3, 4]\n",
            "Number of convolutional filters used :  24\n",
            "Similarity computation (cosine/structured) :  cosine\n",
            "Number of training epochs :  12\n",
            "Batch size used for training :  20\n",
            "Pearson's Correlation Coefficient: 0.2081\n",
            "Mean Squared Error: 1.0427\n",
            "\n",
            "Worst Pair 1:\n",
            "Sentence 1:  ('A man is jumping rope outside',)\n",
            "Sentence 2:  ('A woman is slicing a cucumber',)\n",
            "Difference:  2.66607666015625\n",
            "Model Output:  tensor([3.6661], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 2:\n",
            "Sentence 1:  ('A jet is not flying',)\n",
            "Sentence 2:  ('A dog is barking',)\n",
            "Difference:  2.665893316268921\n",
            "Model Output:  tensor([3.6659], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 3:\n",
            "Sentence 1:  ('A fish is being sliced by a man',)\n",
            "Sentence 2:  ('A cat is jumping into a box',)\n",
            "Difference:  2.6658926010131836\n",
            "Model Output:  tensor([3.6659], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 4:\n",
            "Sentence 1:  ('The man is rock climbing',)\n",
            "Sentence 2:  ('A man is cooking a snake',)\n",
            "Difference:  2.6658871173858643\n",
            "Model Output:  tensor([3.6659], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 5:\n",
            "Sentence 1:  ('A man is jumping rope in a field',)\n",
            "Sentence 2:  ('A woman is slicing a cucumber',)\n",
            "Difference:  2.665604591369629\n",
            "Model Output:  tensor([3.6656], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 6:\n",
            "Sentence 1:  ('There is no hamster singing',)\n",
            "Sentence 2:  ('A person is slicing some onions',)\n",
            "Difference:  2.665588617324829\n",
            "Model Output:  tensor([3.6656], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 7:\n",
            "Sentence 1:  ('Skewers are being placed onto a rack by a woman',)\n",
            "Sentence 2:  ('A dog is jumping high into the air in the country',)\n",
            "Difference:  2.66556453704834\n",
            "Model Output:  tensor([3.6656], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 8:\n",
            "Sentence 1:  ('There is no person slicing a potato',)\n",
            "Sentence 2:  ('A person is performing tricks on a motorcycle',)\n",
            "Difference:  2.665494441986084\n",
            "Model Output:  tensor([3.6655], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 9:\n",
            "Sentence 1:  ('A cat is jumping into a box',)\n",
            "Sentence 2:  ('A young man is talking to a leaf',)\n",
            "Difference:  2.665485382080078\n",
            "Model Output:  tensor([3.6655], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Worst Pair 10:\n",
            "Sentence 1:  ('An elephant is being ridden by a woman',)\n",
            "Sentence 2:  ('A woman is opening a soda and drinking it',)\n",
            "Difference:  2.6654458045959473\n",
            "Model Output:  tensor([3.6654], device='cuda:0')\n",
            "True Label:  tensor([1.], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 1:\n",
            "Sentence 1:  ('A man is holding a small animal in one hand',)\n",
            "Sentence 2:  ('A man is exhibiting a small monkey',)\n",
            "Difference:  0.0011597061157226918\n",
            "Model Output:  tensor([3.6638], device='cuda:0')\n",
            "True Label:  tensor([3.6650], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 2:\n",
            "Sentence 1:  ('A man on inline skates is skating at a skate park',)\n",
            "Sentence 2:  ('A man on inline skates is resting at a skate park',)\n",
            "Difference:  0.016413068771362394\n",
            "Model Output:  tensor([3.6664], device='cuda:0')\n",
            "True Label:  tensor([3.6500], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 3:\n",
            "Sentence 1:  ('People are looking at some costumes gathered in the vicinity of the forest',)\n",
            "Sentence 2:  ('People wearing costumes are gathering in a forest and are looking in the same direction',)\n",
            "Difference:  0.02189945220947287\n",
            "Model Output:  tensor([3.6569], device='cuda:0')\n",
            "True Label:  tensor([3.6350], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 4:\n",
            "Sentence 1:  ('Two women dressed in white and black are sitting on a bench',)\n",
            "Sentence 2:  ('Two people are sitting on a park bench on a sunny day',)\n",
            "Difference:  0.030454149246216033\n",
            "Model Output:  tensor([3.6655], device='cuda:0')\n",
            "True Label:  tensor([3.6350], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 5:\n",
            "Sentence 1:  ('A girl is playing a wind instrument',)\n",
            "Sentence 2:  ('A boy is playing a wind instrument',)\n",
            "Difference:  0.03333325386047381\n",
            "Model Output:  tensor([3.6667], device='cuda:0')\n",
            "True Label:  tensor([3.7000], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 6:\n",
            "Sentence 1:  ('A woman is playing the flute',)\n",
            "Sentence 2:  ('A man is playing the flute',)\n",
            "Difference:  0.033344459533691584\n",
            "Model Output:  tensor([3.6667], device='cuda:0')\n",
            "True Label:  tensor([3.7000], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 7:\n",
            "Sentence 1:  ('A woman is standing at dusk on an almost pristine, snowy road, that is lit only by headlights',)\n",
            "Sentence 2:  ('A man is standing at dusk on an almost pristine, snowy road, that is lit only by headlights',)\n",
            "Difference:  0.03334541320800799\n",
            "Model Output:  tensor([3.6667], device='cuda:0')\n",
            "True Label:  tensor([3.7000], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 8:\n",
            "Sentence 1:  ('No young child is riding a three wheeled scooter down the sidewalk',)\n",
            "Sentence 2:  ('A young child is riding a three wheeled scooter down the sidewalk',)\n",
            "Difference:  0.03336377143859881\n",
            "Model Output:  tensor([3.6666], device='cuda:0')\n",
            "True Label:  tensor([3.7000], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 9:\n",
            "Sentence 1:  ('Two women are standing together and one is looking through binoculars',)\n",
            "Sentence 2:  ('Two men are standing together and one is looking through binoculars',)\n",
            "Difference:  0.03336973190307635\n",
            "Model Output:  tensor([3.6666], device='cuda:0')\n",
            "True Label:  tensor([3.7000], device='cuda:0', dtype=torch.float64)\n",
            "\n",
            "\n",
            "Best Pair 10:\n",
            "Sentence 1:  ('A man is drilling a hole in a piece of wood',)\n",
            "Sentence 2:  ('A man is closing a hole in a piece of wood',)\n",
            "Difference:  0.03337998390197772\n",
            "Model Output:  tensor([3.6666], device='cuda:0')\n",
            "True Label:  tensor([3.7000], device='cuda:0', dtype=torch.float64)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete the above `test` function's implementation. You may want to store various model properties in a file (e.g., the actual fine-tuned embeddings are in a file `blah-blah.txt`; and all the details about its training and other properties may be stored in `blah-blah.properties`).\n",
        "\n",
        "If your `test` function expects such a properties file, make sure that those files are also saved in a human-readable format, and included in your submission.\n",
        "\n",
        "For your four models (as required for grading, described earlier), run the `test` function. Each run should be in a separate code cell. **Do NOT remove the results of these runs.**\n",
        "\n",
        "Similarly, for each one of your four models, return the results of calling the `best` and `worst` functions. You are free to change the signature of these two functions, if needed. Each run should be in a separate code cell, and clearly mention which model's `best` and `worst` are being called. **Do NOT remove the results of these runs.**\n"
      ],
      "metadata": {
        "id": "XVMOjjJ_9lqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Conceptual Questions and Qualitative Analysis\n",
        "\n",
        "**4.1** Write down a brief but precise description of your structured similarity computation. Then, write this similarity using a mathematical formula. Make sure your formula and your implementation are faithful to each other!\n",
        "\n",
        "My similarity function is simple but effective, I get the cosine similarity of my pooled sentence 1 and 2 which is the pooled similarity. I then also get the lexical overlap of sentence 1 and 2 which gives me overlap, IDF_weighted_overlap, content_only_overlap, and content_only_IDF_weighted_overlap. I then just combine all of these together in a tensor and pass it into my linear layer to get a ouput between 1 and 5. I added weights in the formula since orignally I was going to weigh them differently but for now they are just weighted the same.\n",
        "\n",
        "similarity = tanh(pool_similarity * a + overlap * b + IDF_weighted_overlap * c + content_only_overlap * d + content_only_IDF_weighted_overlap * e)\n",
        "\n",
        "**4.2** Identify at least one linguistic pattern in the worst performing sentence-pairs as shown by the call to `worst` for your best performing model (you may need to use a non-default value of `k`). What aspect of your fine-tuning process will you change to improve the predicted scores for sentences that fall into this pattern? Why do you think this change will work to improve the result for these sentence-pairs?\n",
        "\n",
        "It seems that my models like to always guess near 3.6 since that is closest to both 1 and 5. So it performs worse on sentences that are not related to each other. This probably occurs since my model figures out that this can help minimize the loss very fast but then getting the true outputs takes much longer to learn. I can see as I train my model for longer the MSE decreases and the correlation increases. To help improve my model from falling into this pattern I think it may help to also allow tunings on the embeddings so more change is able to propagate backwards and have a larger impact. I think this change will definitley help improve the model and will definitley lead to better results. I think just training the model more will also help it learn more relations between sentences."
      ],
      "metadata": {
        "id": "B_58A6E6FTsr"
      }
    }
  ]
}